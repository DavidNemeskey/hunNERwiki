%
% File acl2012.tex
%
% Contact: Maggie Li (cswjli@comp.polyu.edu.hk), Michael White (mwhite@ling.osu.edu)
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt]{article}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage[utf8]{inputenc}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{XXX}

%\author{Dávid Márk Nemeskey \\
%  Research Institute for Computer Science and Automation \\
%  Hungarian Academy of Sciences \\
%  H-1111 Lágymányosi út 11, Budapest \\
%  {\tt nemeskey.david@sztaki.hu} \\\And
%  Eszter Simon \\
%  Research Institute for Linguistics \\
%  Hungarian Academy of Sciences \\
%  H-1068 Benczúr utca 33, Budapest \\
%  {\tt eszter@nytud.hu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  
\end{abstract}

\section{Introduction}

Named Entity Recognition (NER), the task of identifying Named Entities (NEs) in unstructured texts and classifying them into pre-selected classes, is one of the most important subtasks in many NLP tasks, such as Information Retrieval, Information Extraction or Machine Translation. The NER task was introduced with the 6th Message Understanding Conference (MUC) in 1995 \cite{Grishman:96}. In MUC shared tasks the NER consists of three subtasks: entity names, temporal expressions and number expressions. However there is a general agreement in the NER community about the inclusion of temporal expressions and some numerical expressions such as amounts of money and other types of units, the most studied types are names of persons, locations and organizations. The fourth type, called "miscellaneous'', is used in the CoNLL NER tasks in 2002 \cite{Tjong:02} and 2003 \cite{Tjong:03}, and includes proper names falling outside the three classic types. Since then, MUC and CoNLL datasets and annotation schemes have been the major standards applied in the field of NER. 

The standard datasets are highly domain-specific (mostly newspaper articles about company activities and defense related activities), and are restricted in size, as well. Thus they are not large enough to train and test robust NE-taggers on them. Moreover, merging these datasets to get a bigger training corpus faces NLP researchers with the problem of combining different tagsets and annotation schemes. However, to build a robust supervised machine learning system for NER, large enough, accurately annotated corpora are essential. Annotating large amount of texts with linguistic information is a time-consuming, highly skilled and delicate work. Mainly due to this fact, one of the main challenges in NLP is reducing the annotation cost. 

One approach to reduce the manual intervention required in language resource production, and thus ultimately production costs is automatically generating resources. An other approach to achieve this aim is using collaborative annotation and/or collaboratively constructed resources, such as Wikipedia, Wiktionary or Linked Open Data. Researchers apply the Web-as-corpus approach, often dealing with such datasets as the Google 5G 1T collection or DBPedia.  

%One of the main trends in NLP is the need for reducing the annotation cost. After examining the calls for papers of nowadays conferences and workshops there seems to be three major directions to achieve this aim. 

%The first approach is the application of unsupervised and semi-supervised learning techniques. Researchers following this approach try to not only reduce, but avoid the annotation cost with minimizing the need for annotated data. These methods can be applied to any language or genre for which adequate raw text resources are available. 

In this paper we present a kind of combination of the approaches mentioned above. We built NE-tagged corpora for English and Hungarian -- fully automatically generated from Wikipedia. %Our goal was to create gold standard corpora which can serve as train, devel and test datasets for automatic NE-taggers. 

The paper is structured as follows. XXX In section~\ref{sec:hun}, we show how the algorithm was adapted to Hungarian.


\section{Wikipedia and NER}
% Wikify!
% stuff from Mining Meaning from Wikipedia
%NER and Wikipedia

Wikipedia is a goldmine of information; it is applied for several NLP tasks, e.g.~word sense disambiguation, ontology and thesaurus building, multilingual Information Retrieval, and Question Answering, as Medelyan et al.~\shortcite{Medelyan:09} point out in their fully detailed survey about mining meaning from Wikipedia. 

Wikipedia is recognized as one of the largest available collection of entities, and also as an external knowledge to improve the accuracy of NER. The most obvious utilization of Wikipedia for NER is extracting gazetteers containing person names, locations or organizations (e.g. Toral and Munoz \shortcite{Toral:06}). Creating dictionaries of entities is also a common step of named entity disambiguation \cite{Bunescu:06,Cucerzan:07}. Not only supervised systems use lists generated from Wikipedia: Nadeau et al. \shortcite{Nadeau:06} use similar lists in an unsupervised NER system. 

Wikipedia's knowledge may be incorporated as features in NER learning, e.g. Kazama and Torisawa \shortcite{Kazama:07} showed that automatic extraction of category labels from Wikipedia improves the accuracy of a supervised NE-tagger. 

A third approach to improve NER with Wikipedia is the automatic creation of training data. Richman and Schone \shortcite{Richman:08} built corpora for less commonly taught languages annotated with NE-tags. They used the inherent category structure of Wikipedia to determine the NE-type of a proposed entity. Nothman et al.~\shortcite{Nothman:08} used a similar method to create a NE-annotated text in English. They transformed the Wikipedia links into NE-annotations by classifying the target articles into standard entity classes. Their approach to classification is based primarily on category head nouns and definitional opening sentences of articles. 

Despite the aforementioned methods, we applied a new approach to recognize and classify NEs in the corpora generated from Wikipedia. We used the DBpedia ontology classes to map Wikipedia categories to standard NE tags (see more details in section \ref{dbpedia}). 

To our knowledge, such type of automatically built corpora are not freely available, with the only one exception of the Semantically Annotated Snapshot of the English Wikipedia \cite{Zaragoza:07}. It provides a wide range of linguistic information: POS tags, dependency labels, WordNet super senses and NE annotation according to WSJ and CoNLL tagsets. However, they followed a different way: the NEs were tagged by open source softwares. Our corpora will be available under the same license as Wikipedia's one (Creative Commons Attribution-Sharealike 3.0 Unported License).

Thanks for the huge amount of Wikipedia articles, the possibility of building large enough corpora is given for less resourced languages, as well. Since our method is mainly language-independent, it can be easily applied for languages which are structurally differing from English. Hungarian is a highly agglutinative language, with free word order and other particularities detailed later in section \ref{sec:hun}. There are human annotated CoNLL-style datasets, but this presented here is the first large enough automatically NE-annotated corpus for Hungarian. 

\section{Creating the English Corpus}  % Dzsííízösz
\label{sec:create}

% Introduction: Wikipedia + DBpedia

Wikipedia proves to be a rich source of information about its entities; aside from the article text, a huge amount of data is encompassed in infoboxes, templates and the category structure. Since our goal is to create a NE-annotated corpus containing large amount of texts, our interest is limited to the article text. Templates are removed from the articles; however, the names of the templates have been retained for XXX, but all data (including infobox) 

Beside regular articles, Wikipedia also contains redirect and disambiguation pages, which require special treatment. Redirect pages are empty, and are safe to ignore altogether. Disambiguation pages contain text, usually in the form of an enumeration of page links with a short description. Full sentences are scarce, and the structure of these pages makes it hard to extract valid training sentences from them. Therefore, disambiguation pages are dropped from the corpus, as well.

The English corpus is based on the Wikipedia snapshot as of XXX. The XML files were parsed with the mwlib (CITE) parser. The raw text was tokenized with a modified version of the XXX sentence tokenizer in NLTK (CITE). The words were lemmatized with XXX and POS tagged with HunPos \cite{Halacsy:07}. 

%Chunking was performed with HunChunk (CITE). Experiments have shown that HunPos obtains STAT f-score on the Brown corpus; HunChunk performs at around STAT, CITE?.

\subsection{Extracting and Filtering Sentences}

A NER corpus consists of sentences that accurately tag NEs occurring in them. In order to automatically prepare such sentences, two tasks need to be performed: identifying the entities in the sentence and tagging them with the correct tag. Sentences, for which accurate tagging could not be accomplished, must be removed from the corpus. In this section, we describe how these steps were implemented, as well as the post-filtering we employed to dispose of correctly tagged, but unusable training sentences.

\subsubsection{Identifying the entities}
% TODO: identification and tagging happens in parallel, else the recall oriented approach cannot be explained.

Our approach to entity identification relies on Wikipedia cross-references found in the article text. We assume that individual Wikipedia articles describe named entities. A link to an article can then be perceived as a mapping that identifies its anchor text with a particular entity. However, there are exceptions to this rule. Disambiguation pages are used when the page title is ambiguous, and can refer to several entities. If a link points to a disambiguation page, we acknowledge the anchor text as an entity, but mark it unknown. A similar measure is taken if the link target XXX does not exist. Links to redirect pages are resolved to point instead to the redirect target, after which they are handled as regular cross-references.

However, not all entities are marked with links. Authors may neglect to add XXX references to their text. Furthermore, in a Wikipedia article XXX, typically only the first occurrence of a particular entity is linked to the corresponding page. Subsequent mentions are unmarked and often incomplete -- e.g.~family names are used instead of full names. We devised XXX two methods to account for unmarked entities: one that emphasizes XXX precision, and the other, recall. XXX

% Or NPP

The precision-oriented approach seeks to minimize the risk of words getting incorrectly tagged XXX in the corpus. Names and their parts mostly start with capital letter, thus 
%A short study of the CoNLL annotation guidelines reveals that the words in the entities of interest to us all start with capital letter. According to this method XXX, 
all sentences that contain capitalized words, but are not part of the anchor text of a link, XXX were discarded. The rationale behind this approach is that the sheer size of Wikipedia guarantees that we end up with a sizeable corpus even with such a strict policy. XXX

The recall-oriented method tries to associate unmarked entities with entities already identified on the same page. Only if the attempt fails is the sentence that contains the words discarded. 
%Ez utóbbi biztosan jó mondat?
This approach aims to generate more sentences with as little loss of precision as possible. 

The algorithm maintains a list of entities already indentified on the page. The list includes:
\begin{itemize}
\item The page title; % if it is in DBpedia
\item Title of redirect pages that target the current page; % TODO
\item Entities marked with links in the text thus far.
\end{itemize}

When an unmarked entity is encountered, the list is consulted to find an item that contains it in its entirety; i.e.~all of its words, in the same order. If either exactly one such item is found, or the matching entities in the list all have the same NE tag, the unmarked entity is tagged accordingly. Otherwise the sentence is discarded.

%Both approaches are evaluated in the next chapter.

%Szerintem ide kéne a DBpedia--CoNLL mapping
We use DBpedia as 

Only sentences for whose entities the correct category could be obtained from DBpedia are added to the corpus; those that contain unknown entities are discarded. 

%\subsubsection{Tagging}

%TODO: merge with Entity Identification
%megtörtént

\subsubsection{Exceptions}

Strictly speaking, our original assumption of equating Wikipedia articles with named entities is not valid: many pages describe common nouns (Book, Aircraft), numbers (1, 42), currencies (Euro, Dollar) or other concepts that fall outside the scope of NER. In this section, we describe the measures taken to avoid errors introduced by these false entities. We shall modify the two approaches -- precision and recall-oriented -- accordingly.

Proper entities that are not deemed NE under the ConLL scheme cause XXX little concern: they are either missing from DBpedia altogether, or we can map the classes they belong to to the NER category \texttt{0} (not an entity). This "ontological filtering" ensures that we need no adjustment XXX to our original method.

Links to number and common noun pages are handled differently by the two approaches. The precision-oriented method treats them similarly to links to unknown entities and discards the sentences in which they occur. The recall-oriented approach tries to distinguish them from links to entities so that otherwise valid training sentences can be retained XXX. The method is as follows XXX.
\begin{itemize}
\item Common nouns are filtered by POS category: links that contain no proper nouns are ignored.
\item Numbers behave more like proper nouns in that they can refer to entities, such as 737 to the Boeing 737. Therefore, links to number pages are filtered by the surface form of the page title.
\end{itemize}

Time expressions require special attention due to two reasons. First, Wikipedia has pages for years, months and concrete dates, therefore our algorithm might mistake them for entities. Second, as dates are made up of numbers and proper nouns, they can be mistaken for unmarked entities. To avoid these errors, we have create a set of regular expressions that recognize dates and time expressions and changed the recall-oriented method to not treat them as entities.

from \cite{Nothman:09}: Exceptions are made for common titlecase words, e.g. I, Mr., June, and sentence-initial words. 

% TODO: monetary, etc. units?

\subsubsection{Filtering}

There are many incomplete sentences in the Wikipedia text: image captions, enumerations items, contents of table cells, etc. On the one hand, these sentence fragments may prove to be of too low quality to be of any use in the traditional NER task. On the other hand, they could prove to be invaluable when training a named entity recognizer for user generated content. We have added a post-filtering step to recognize and label these fragments, so that users of the corpus can decide what to do with them. XXX

Manual inspection has found two conditions that can be used to detect fragments. A sentence is labelled as such if it either lacks a punctuation mark at the end or it contains no finite verbs. Our final corpus contains STAT full sentences and STAT fragments.
% TODO: Last sentence to evaluation? But then this whole subsection is too short.

% 4. Test sentence extraction?
%   a. unknown links, no unlinked names
%   b. might be good for quality evaluation / recursive tagging (next paper?)
% 5. teaching the original DBpedia categories?

\subsection{The DBpedia -- ConLL mapping}
\label{dbpedia}
% Introduction + entity -> class -> category.


The classes in DBpedia are organized into a type hierarchy, available as an OWL (CITE) ontology. The ontology contains the 320 most frequent categories of Wikipedia, arranged into a taxonomy under the base class \texttt{owl:Thing}. Most of the classes belong into the 5 largest sub-hierarchies (here named after their topmost classes XXX): \texttt{Agent} (which is further divided into \texttt{Person} and \texttt{Organisation}), \texttt{Event}, \texttt{Place}, \texttt{Species} and \texttt{Work}. The taxonomy is rather flat: the top level contains 44 classes and there are several nodes with a branching factor of 20.

It is not difficult to see the parallels XXX between the DBpedia sub-hierarchies \texttt{Person}, \texttt{Organisation} and \texttt{Place} and the ConLL NER categories \texttt{PER}, \texttt{ORG} and \texttt{LOC}. The fourth category, MISC is more elusive; according to the ConLL NER annotation guide (CITE?: http://www.cnts.ua.ac.be/conll2003/ner/annotation.txt), the sub-hierarchies \texttt{Event} and \texttt{Work} belong to this category, as well as various other class outside the main sub-hierarchies. 

While the correspondence described above holds for most classes in the XXX sub-hierarchies, there are some exceptions. For instance, the class \texttt{SportsLeague} is part of the \texttt{Organisation} sub-hierarchy, but in the ConLL corpus, sport leagues are tagged as MISC. Similarly, the class \texttt{SportsTeamSeason} is under \texttt{Event}, by which account it would be classified as MISC; however, ConLL does not recognize such entities. To account for these misclassification, as well as the miscellaneous classes outside the main hierarchies, we have introduced a mapping mechanism. We created a file of DBpedia class -- NER category mappings. Whenever an entity is evaluated, we look up its class, as well as the ancestors of its class, in the mapping, and assign to the entity the category of the class that matches it most closely -- i.e. the category of the entity's class, if it is in the list, or that of its closest ancestor. XXX!!! If no match is found, the entity is tagged with \texttt{O}. Since we take advantage of the inheritance hierarchy, the mapping list remains short: it contains only the root classes of the main sub-hierarchies, exceptions of the like XXX mentioned above and the various classes that belong to the MISC category according to the ConLL annotation guideline.

As of version 3.7, the DBpedia ontology allows multiple superclasses, making the hierarchy a directed acyclic graph (CITE). This introduces the problem of selecting the right superclass, and hence, ConLL mapping for classes with more than one parent. In version 3.7, the only such class is \textit{Library}, which can be traced back to both \textit{Place} and \textit{Organisation}. There are two ways to tackle this problem. The first is to follow the DBpedia guidelines and only retain the first parent. The second method is manually deciding the right NER mapping for the ambiguous XXX class and adding it to the mapping file. The first approach, unfortunately, already fails to assign the correct ConLL NER tag to our only example. The reason behind this failure is that multiple parents in an ontology show an inherent ambiguity in the class in question: it is entirely natural to think of a library as a location as well as a public organization. DBpedia's guidelines prefers Organisation; ConLL's LOC. Therefore, only the second, manual approach ensures correct tagging. XXX

The full mapping can be found in LINK Appendix A (XXX or not).

In Wikipedia the nationalities (e.g. American and Australian) links to the corresponding places (America and Australia, respectively). However, in CoNLL scheme these types of entities take parts of MISC category. XXX Postprocessing: list look-up (nationalities, adjective forms of person who live in the corresponding country); if the name is POS-tagged as adjective, then it is not a NE

%nem biztos, hogy ide kell
Link boundaries need to be adjusted, e.g. to remove excess punctuation.

\section{Creation of the Hungarian Corpus}
\label{sec:hun}

The procedure described in the last section was used to generate the Hungarian corpus as well. However, the differences between English and Hungarian prompted XXX several modifications in its steps. In this section we describe how the algorithm was adapted to Hungarian. XXX!!!

in tool a

\subsection{Tools}

While NLTK has excellent tools for English, Hungarian is not supported. Therefore, we worked with the tools developed by the Hungarian computational linguist community. For sentence tokenization we used an in-house statistical chunker tailored for Hungarian. It has been trained on the Szeged corpus and it handles the peculiarities of Hungarian ortography, such as the periods placed after numbers in date expressions. Although HunPos, as its name implies, is fully capable of POS tagging Hungarian text, we opted for Hunmorph (CITE), a morphological analyzer that, apart from PAS tagging, also lemmatizes the words. Hundisambig (CITE) was used to select the right analysis based on the word context\footnote{Incidentally, Hundisambig relies on HunPos for its HMM model.}.

The Hungarian part-of-speech taggers output KR codes (CITE). In addition to the POS category, the KR code also includes inflection information, such as the number, person and case XXX, which makes it much better suited to agglutinative languages than Penn Treebank POS tags. One shortcoming of the KR code is that it does not differenciate between common and proper nouns. Since in Hungarian only proper nouns are capitalized, we can usually decide whether a noun is proper or not based on the initial letter. However, this rule cannot be used if the noun is at the beginning of a sentence, as words at the beginning of sentences are always capitalized. Hence, all sentences that begin with nouns have been removed from the corpus.

\subsection{Data Availability}

For some of the bigger languages, DBpedia has internationalized "chapters" -- data dumps based on the Wikipedia of that particular language. Hungarian, being a relatively small language XXX, does not have a separate chapter. Instead, the Hungarian entity list comprises of the pages in the English list that have their equivalents in the Hungarian Wikipedia. Two consequences follow. First, in order to identify which pages denote entities in the Hungarian Wikipedia, an additional step is required, in which the Hungarian equivalents of the English pages are added to the entity list. The English titles are retained, as due to the relatively small size of the Hungarian Wikipedia, in-article links sometimes point to English articles. Second, entities without a page in the English Wikipedia are absent from the entity list. This gives rise to two potential problems. One is that compared to English, the list is relatively shorter: the entity/page ratio is 12.12\%, as opposed to the 37.66\% of the English Wikipedia. The other, since mostly Hungarian people, places and organizations are missing, a NER tagger that takes the surface forms of words into account might be mislead as to the language model of entity names. To overcome these problems, the list has to be extended with Hungarian entity pages that do not have a corresponding English page. We leave this for future work.

In addition to differences in grammar, 

The Szeged NER guideline relies heavily on the rules of capitalization to decide which words should be marked as named entities. In Hungarian, apart from the first word in the sentence, only proper nouns are capitalized.
Thus, the following concepts are not considered named entities in Hungarian XXX:
\begin{itemize}
\item Names of languages and ethnic groups
\item Words created from proper nouns through derivation, most prominently
\item Adjectival form of location names
\item Months % angolban sem
\end{itemize}

filtering: az igés szűrő itt nem lesz

most van: 102 millió szó a disambiguation pages-zel együtt
az angol: 1,7 milliárd

according to the Szeged NER guideline

a tulajdonnevekhez kötőjellel kapcsolódó köznevek kérdésesek (pl. SAP-szoftvercég, Matáv-csoport)

% Good for middle-sized languages
% Inconsistent template names, esp. across languages
% Comparison of the Hungarian DBpedia vs English + mapping
% Not all entities are written w/ capital letters => we need lists.

\section{Corpus Description}

% TODO: pls írd meg az első pár mondatot és nevezd át a sectiont! 

The corpora are available in the tab separated format used by ConLL (XXX, CITE). In this format, one line contains one word, its linguistic features and the named entity tags. Sentence boundaries are marked by empty lines. The linguistic features include the lemmatized form of the word, its part-of-speech and chunk tags. Two named entity tags are included with each word: one is the most specific DBpedia category it belongs to, if any; the other is the ConLL named entity tag. While the named entity tags can be considered "silver", the linguistic features are provided on a "best-effort" basis.

The corpus is available under the Creative Commons Attribution-Sharealike 3.0 Unported License (CC-BY-SA), the same license under which the text of Wikipedia is released. The data files can be freely downloaded from ...\footnote{To be filled in in the camera-ready version.}.


\section{Evaluation}

% vs. ConLL / Szeged
% Corpus sizes, performance, tagging conventions

% Hun -> not utf-8, loss of precision?

Having the obvious advantages, an automatically generated corpus can not serve as a gold standard dataset. Then what can we do with these so called "silver standard corpora" (SSC)? They can be very useful for improving NER in more ways: (a) for less commonly used languages, they can serve as training corpora in absence of gold standard datasets; (b) they can serve as (supplementary or independent) training sets for domains differing from newswire; (c) they can be sources of huge entity lists, and (d) feature extraction. 

Sentences without any entities are retained as well, so that the corpus mirrors (XXX the distribution of entities in the corpus mirrors their distribution in Wikipedia?) the distribution of entities in Wikipedia as closely as possible; this is also in line with how the ConLL corpus is assembled. (XXX or is it)

measuring Wikipedia's inconsistency using n-gram tag variation (\cite{Nothman:09})
ez jó mérőszám lehet a gold standard korpuszok összehasonlítására
használhatjuk fordítva is: ha egy adott nucleus ugyanolyan n-gram környezetben nagyon sokszor van X-ként taggelve, és kevésszer Y-ként, akkor az Y-okat is átcsinálhatnánk X-szé


\section{Conclusions and Future Work}

az eredeti DBPedia osztályokat is megtarthatnánk, hiszen nem mindig célravezető a CoNLL osztályok használata, mivel azok elég tág kategóriák, amikbe olyan dolgok is tartoznak, amik nagyon eltérőek is lehetnek

később majd kirakjuk a scripteket is, és akár újra és újra frissíthető lesz

%\section*{Acknowledgments}

%The authors are grateful to Attila Zséder and Gábor Recski for their respective work on Wikipedia parsing and {\tt hunner}.
% + CE(A?)SAR

\begin{thebibliography}{}

\bibitem[\protect\citename{Halácsy \bgroup et al.\egroup}2007]{Halacsy:07}
P. Halácsy, A. Kornai and Cs. Oravecz.
\newblock 2007.
\newblock Hunpos -- an open source trigram tagger. 
\newblock In: {\em Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics}, pages 209--212.

\bibitem[\protect\citename{Zaragoza \bgroup et al.\egroup}2007]{Zaragoza:07}
H. Zaragoza and J. Atserias and M. Ciaramita and G. Attardi.
\newblock 2007.
\newblock Semantically Annotated Snapshot of the English Wikipedia v.1 (SW1).
\newblock \url{http://www.yr-bcn.es/semanticWikipedia}

\bibitem[\protect\citename{Tjong Kim Sang}2002]{Tjong:02}
Erik F. Tjong Kim Sang.
\newblock 2002.
\newblock Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition.
\newblock In: {\em Proceedings of the 6th Conference on Natural Language Learning}, pages 1--4, Taipei, Taiwan.

\bibitem[\protect\citename{Tjong Kim Sang and De Meulder}2003]{Tjong:03}
Erik F. Tjong Kim Sang and Fien De Meulder.
\newblock 2003.
\newblock Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. 
\newblock In: {\em Proceedings of the 7th Conference on Natural Language Learning}, pages 142--147, Edmonton, Canada.

\bibitem[\protect\citename{Grishman and Sundheim}1996]{Grishman:96}
Ralph Grishman and B. Sundheim.
\newblock 1996.
\newblock Message Understanding Conference -- 6. 
\newblock In: {\em Proc. International Conference on Computational Linguistics}.

\bibitem[\protect\citename{Nothman \bgroup et al.\egroup}2009]{Nothman:09}
\newblock 2009.
\newblock Analysing Wikipedia and Gold-Standard Corpora for NER Training.
\newblock In: {\em Proceedings of the 12th Conference of the European Chapter of the ACL}, pages 612--620.

\bibitem[\protect\citename{Richman and Schone}2008]{Richman:08}
Alexander E. Richman and Patrick Schone.
\newblock 2008.
\newblock Mining Wiki Resources for Multilingual Named Entity Recognition.
\newblock In: {\em Proceedings of ACL-08: HLT}, pages 1--9.

\bibitem[\protect\citename{Nothman \bgroup et al.\egroup}2008]{Nothman:08}
Joel Nothman, James R. Curran, and Tara Murphy.
\newblock 2008.
\newblock Transforming Wikipedia into Named Entity Training Data.
\newblock In: {\em Proceedings of the Australasian Language Technology Workshop}, Vol 6., pages 124--132.

\bibitem[\protect\citename{Kazama and Torisawa}2007]{Kazama:07}
Jun'ichi Kazama and Kentaro Torisawa.
\newblock 2007.
\newblock Exploiting Wikipedia as External Knowledge for Named Entity Recognition.
\newblock In: {\em Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning}, pages 698--707.

\bibitem[\protect\citename{Medelyan  \bgroup et al.\egroup}2009]{Medelyan:09}
Olena Medelyan, David Milne, Catherine Legg, and Ian H. Witten.
\newblock 2009.
\newblock Mining meaning from Wikipedia.
\newblock {\em International Journal of Human-Computer Studies}, 67: 716--754.

\bibitem[\protect\citename{Toral and Munoz}2006]{Toral:06}
A. Toral and R. Munoz.
\newblock 2006.
\newblock A proposal to automatically build and maintain gazetteers for Named Entity Recognition by using Wikipedia.
\newblock In: {\em EACL 2006}.

\bibitem[\protect\citename{Bunescu and Pasca}2006]{Bunescu:06}
B. Bunescu and M. Pasca.
\newblock 2006.
\newblock Using encyclopedic knowledge for named entity disambiguation.
\newblock In: {\em Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics}, pages 9--16.

\bibitem[\protect\citename{Cucerzan}2007]{Cucerzan:07}
S. Cucerzan.
\newblock 2007.
\newblock Large-scale named entity disambiguation based on Wikipedia data.
\newblock In: {\em Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning}.
\newblock Prague, Czech Republic, June 2007. pages 708--716.

\bibitem[\protect\citename{Nadeau  \bgroup et al.\egroup}2006]{Nadeau:06} 
David Nadeau, Peter D. Turney and Stan Matwin.
\newblock 2006.
\newblock Unsupervised named entity recognition: Generating gazetteers and resolving ambiguity. 
\newblock In: {\em Proceedings of the 19th Canadian Conference on Artificial Intelligence}, volume 4013 of {\em LNCS}, pages 266--277.

\bibitem[\protect\citename{Bizer \bgroup et al.\egroup}2009]{Bizer:09} 
Christian Bizer, Jens Lehmann, Georgi Kobilarov, Sören Auer, Christian Becker, Richard Cyganiak, Sebastian Hellmann.
\newblock 2009.
\newblock DBpedia -- A Crystallization Point for the Web of Data. 
\newblock In: {\em Journal of Web Semantics: Science, Services and Agents on the World Wide Web}, Issue 7, pages 154--165.



\end{thebibliography}

\end{document}
