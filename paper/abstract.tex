% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science, version 1.1
\documentclass[10pt, a4paper]{article}
\oddsidemargin 4mm
\evensidemargin 4mm
\usepackage{lrec2006}
\usepackage[colorlinks,urlcolor=blue,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
%\usepackage[hungarian]{babel}
%\selectlanguage{hungarian}
%
\begin{document}

\title{Rapid creation of large-scale corpora and frequency dictionaries}

\name{Attila Zs\'eder$^1$, G\'abor Recski$^{1}$, D\'aniel Varga$^2$, Andr\'as Kornai$^1$}
\address{$^1$Computer and Automation Research Institute\\Hungarian Academy of Sciences\\{\tt\{recski,zseder,kornai\}@sztaki.hu}\\
$^2$Media Education and Research Center\\Budapest University of Technology and Economics\\{\tt daniel@mokk.bme.hu}
}

%beigerunk ket tucatot es a szoftvert
%sztaki szotar osszes nyelve is minusz amiben nem nyomjuk le Baronit 
%TODO es textcat is, hivatkozassal
%Baroniekrol ne seggnyalast, hanem angol nemet olaszra oket "for the largest languages"
%TODO miert mas ez mint a wac pipeline, Daniel?
%TODO ha minden említett korpuszhoz lenne hivatkozás, az plusz féloldalnyi irodalomjegyzék

\abstract{We describe, and make public, large-scale language resources and the
  toolchain used in their creation, for several medium density European languages}



\maketitleabstract

Using the web as a source of linguistic data is by no means a new idea: the
first efforts in this direction were made over a decade ago (Resnik 1999,
Varantola 2000, Davies 2001)\nocite{Resnik:1999}, and in 2003 {\it
  Computational Linguistics} devoted an entire special issue to the subject
(Kilgarriff and Grefenstette 2003).\nocite{Kilgarriff:2003} It is all the more
surprising that a decade later language resources based on web corpora are,
aside from a handful of major languages, still largely missing. Perhaps
unsurprisingly, the largest European languages (English, German, and French)
fared best, as gigaword corpora and frequency dictionaries have already been
created for these as part of the \texttt{Wacky} project (Baroni and Kilgarriff
2006, Baroni et al. 2009)\nocite{baroni2009wacky}\nocite{baroni2006large}. The
respective corpora \texttt{ukWac, deWac}, and \texttt{frWac} contain billions
of tokens and the derived frequency dictionaries (though not the corpora
themselves) are freely available for download. Another discernible tendency of
the decade is the rise of national corpora (Tadi\'c
2002,\nocite{tadic2002building} Przepi{\'o}rkowski et
al. 2008,\nocite{przepiorkowski2008towards} Kucera
2002).\nocite{kucera2002czech} Remarkably, the fruits of such state- and
EU-funded efforts tend to remain behind query interface walls: individual
results pertaining to individual words may be accessible, but the data as a
whole is not available for download (see Table \ref{table:access}).

Since the fundamental idea is quite sound for medium- and small-density
languages as well, we would have expected there to be a significant selection
of linguistic data accessed by means of crawling. But for most medium European
languages frequency dictionaries derived from gigaword corpora are still
unavailable. Our goal in this paper is to show that a fairly simple pipeline
of entirely generic crawling and only minimally language-specific data
cleaning steps can produce remarkable basic language resources, in particular
corpora and frequency dictionaries, that are considerably larger than those
currently available.
%We also present a toolchain which uses exclusively open-source technology and which will be made available to the public in its entirety.
For seven languages the frequency dictionaries are already publicly accessible
and over two dozen will be available by the time of the final paper. By that
time we shall also release the open-source toolchain we created.

First we describe the process of creating corpora using a combination of
open-source NLP tools. Since the well-known
\texttt{Heritrix}\footnote{\href{http://crawler.archive.org}{http://crawler.archive.org}}
crawler tends to slow down and even halt after several days of operation,
which makes significant human supervision essential to its operation, we use
the less well known but considerably faster and autonomous {\tt wire} crawler
(Castillo et al 2005).\nocite{castillo2005wire} In all fairness, what makes
{\tt heretrix} slow is precisely its excellence in the task it was designed
for, building complete snapshots, and what makes {\tt wire} fast is the
radical pruning of sites (e.g. that no timed-out site is tried a second time),
a strategy only made possible by the fact that all we need are large samples,
completeness is not a goal. In our experience, {\tt wire} will easily sustain
10-20 GB/day throughputs, while {\tt heretrix} will slow down by an order of
magnitude after the first day.

Subsequent stages of the pipeline used in creating the \textit{*Wac} corpora
are described in detail in the papers of Baroni and his co-workers cited
above, but only a few components of the toolchain are publicly
accessible. Therefore, we created our own tools (Hal\'acsy et
al. 2008)\nocite{halacsy2008parallel}, and made sure their runtime is
negligible compared to the time it takes to crawl. By now, our tools are
capable of processing a week's worth of crawl data in a matter of hours, while
the filtering process used in the \texttt{Wacky} project is reported to have
taken several days for each language.  All components of our toolchain are
open source (LGPL) and the packaged pipeline will be freely downloadable by
the time the final paper is submitted.

For seven European languages (Catalan, Czech, Croatian, Dutch, Italian,
Polish, Spanish) we have already gathered enough data to create large-scale
corpora. For these languages, table \ref{table:existing} shows the size of the
largest existing corpora that we know of. Table \ref{table:access} shows the
availability of these resources. Remarkably, only {\tt itWaC} is actually 
available without limitations. 

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
corpus & download & search \\%& URL\\
\hline
CUCWeb & no & yes \\
Czech National Corpus& no & yes \\%& \href{http://ucnk.ff.cuni.cz/}{http://ucnk.ff.cuni.cz/} \\
Croatian National Corpus& no & yes\\% & \href{http://www.hnk.ffzg.hr/}{http://www.hnk.ffzg.hr/} \\
Dutch Parallel Corpus & no & no \\%&\href{http://www.kuleuven-kortrijk.be/DPC}{http://www.kuleuven-kortrijk.be/DPC} \\
itWaC & yes & no \\%& \href{http://wacky.sslmit.unibo.it/}{http://wacky.sslmit.unibo.it/} \\
Polish National Corpus & no & yes\\% &\href{http://nkjp.pl}{http://nkjp.pl} \\
Corpus del Espanol & no & yes \\%&\href{http://www.corpusdelespanol.org/}{http://www.corpusdelespanol.org/} \\
\hline
\end{tabular}
\caption{Corpus access}
\label{table:access}
\end{table}

\begin{table*}[ht]
\centering
\begin{tabular}{|l|l|r|l|l|}
\hline
Language & Largest corpus & tokens & Reference & URL\\
\hline
Catalan & CUCWeb & 166m & Boleda et al. 2006\nocite{boleda2006cucweb} & \href{http://ramsesii.upf.es/cucweb/}{http://ramsesii.upf.es/cucweb/}\\
Czech & Czech National Corpus& 1300m & Kucera 2002\nocite{kucera2002czech} & \href{http://ucnk.ff.cuni.cz/}{http://ucnk.ff.cuni.cz/}\\
Croatian & Croatian National Corpus& 100m & Tadic 2002\nocite{tadic2002building}& \href{http://www.hnk.ffzg.hr/}{http://www.hnk.ffzg.hr/} \\
Dutch & Dutch Parallel Corpus & 10m & Paulussen et al. 2006\nocite{paulussen2006dutch} & \href{http://www.kuleuven-kortrijk.be/DPC}{http://www.kuleuven-kortrijk.be/DPC} \\
Italian & itWaC & 1900m & Baroni et al. 2009& \href{http://wacky.sslmit.unibo.it/}{http://wacky.sslmit.unibo.it/} \\
Polish & Polish National Corpus & 1200m &Przepi{\'o}rkowski et al. 2008\nocite{przepiorkowski2008towards}& \href{http://nkjp.pl}{http://nkjp.pl} \\
Spanish & Corpus del Espanol & 100m & Davies 2001\nocite{davies2001creating}& \href{http://www.corpusdelespanol.org/}{http://www.corpusdelespanol.org/}\\
\hline
\end{tabular}
\caption{Existing corpora}
\label{table:existing}
\end{table*}

In our own work, first we collected HTML documents from the relevant top-level domains
(\texttt{.cat}, \texttt{.cz}, \texttt{.hr}, etc.). The data obtained was then
converted to raw text, split into sentences and tokenized. For language
detection we used Gertjan van Noord’s n-gram language classifier
\texttt{textcat}\footnote{\href{http://www.let.rug.nl/\~vannoord/TextCat}{http://www.let.rug.nl/$\sim$vannoord/TextCat}}
To further clean our data, we passed each set of documents through the
\texttt{hunspell} (Németh et al. 2004)\nocite{nemeth2004leveraging}
spellchecker and discarded those in which the ratio of misspelled words were
above some language-specific threshold. This kind of filtering is in itself an
efficient method for language identification and also reduces all kinds of
noise in raw text obtained by crawling the web. This produced corpora with
sizes in the hundred millions (see table \ref{table:size}). Since further
crawling of the web is currently in progress, we shall most probably provide
gigaword corpora in all of these, as well as another 12-15 languages.

%The number of tokens and lemmas for each language can be seen in table \ref{table:size}. 

\begin{table}
\centering
\begin{tabular}{|l|r|r|}
\hline
Language & tokens & lemmas\\
\hline
Catalan & 492m & 62k\\
Czech & 655m & 189k\\
Croatian & 382m & 168k\\
Dutch & 463m & 95k\\
Italian & 944m & 79k\\
Polish & 366m & 152k\\
Spanish & 528m & 56k\\
\hline
\end{tabular}
\caption{Corpus sizes by language}
\label{table:size}
\end{table}

The next step involved counting the number of occurences of each token and
thus creating a frequency dictionary of all word forms of a language that are
present in the data gathered. We then passed the lists through
\texttt{hunspell} for the purpose of stemming and summed the frequency of
stems in order to create a frequency dictionary of lemmas.

In average, \texttt{hunspell} found some 10 percent of all word forms
ambiguous and returned multiple stems. The frequency dictionaries we created
contain two figures for each stem: one was obtained by choosing the shortest
possible stem for each ambiguous form, while the other is the result of
summing the frequencies of all tokens for which the given stem was among the
options returned by \texttt{hunspell}. Both the frequency dictionaries and the
tokenized corpora are available for download at
\href{http://hlt.sztaki.hu/resources}{http://hlt.sztaki.hu/resources}. The
ratio of tokens unrecognized by \texttt{hunspell} can be seen in table
\ref{table:recognized}.

\begin{table}
\centering
\begin{tabular}{|l|r|r|r|}
\hline
Language & tokens & unknown & ratio\\
\hline
Catalan & 492m & 84m & $17.04\%$\\
Czech & 655m & 79k & $12.00\%$ \\
Croatian & 383m & 74m & $19.24\%$\\
Dutch & 463m & 58m & $12.50\%$\\
Italian & 944m & 131k & $13.83\%$\\
Polish & 366m & 55m & $14.93\%$\\
Spanish & 528m & 72m & $13.61\%$\\
\hline
\end{tabular}
\caption{Ratio of unrecognized word forms}
\label{table:recognized}
\end{table}

The pipeline we have created enables us to efficiently create good quality
corpora and frequency dictionaries as newer data is crawled. For Catalan, 
Croatian, Dutch, and Spanish we have already created the largest freely
downloadable corpus, and in the final paper we hope to present figures that 
improve on the current state of the art for several more languages. 

%In order to create good quality corpora from the available text we passed
%each set of documents through the \texttt{hunspell} spellchecker and
%discarded those in which the ratio of misspelled words were above some
%language-specific threshold. This kind of filtering is an efficient method
%for language identification and reduces all kinds of noise in raw text
%obtained by crawling the web. The size of the corpora thus obtained can be
%seen in table \ref{table:cleanedSize}.

%These figures show that the corpora obtained are larger than the Croatian


%Ezért lesz különösen kurvajó a gyakorisági szótár, ezért lemmatizálunk! Olykor morfológiailag elemzünk! Végül konklúzió!

\bibliographystyle{lrec2006}
\bibliography{webcorp}{}

\end{document}


%Catalan & 490901211 & \\
%Czech & & \\
%Croatian & 382167628 & \\
%Dutch & 463162799 & \\
%Italian & & \\
%Polish & 365751955 & \\
%Spanish & 528249527 & \\
