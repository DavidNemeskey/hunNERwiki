\documentclass{llncs}

\usepackage[a4paper,includeheadfoot,top=1.65in,bottom=1.65in,left=1.73in,hcentering]{geometry}
\usepackage[pdftex]{graphicx}

\usepackage[utf8]{inputenc}

\usepackage[hungarian]{babel}
\selectlanguage{hungarian} 

\begin{document}

\pagestyle{myheadings}
\def\leftmark{{\rm IX. Magyar Sz\'am\'\i t\'og\'epes Nyelv\'eszeti Konferencia}}
\def\rightmark{{\rm Szeged, 2013. január 7-8.}}

\setcounter{page}{3}

\title{\ \break Automatikusan tulajdonnév-annotált korpuszok előállítása a Wikipédiából}
\author{Nemeskey Dávid Márk\inst{1}, Simon Eszter\inst{2}}
\institute{
\inst{}%
MTA SZTAKI \break
1111 Budapest, Lágymányosi utca 11., e-mail:nemeskey@sztaki.mta.hu \break
\and
\inst{}%
MTA Nyelvtudományi Intézet \break
1068 Budapest, Benczúr u. 33., e-mail: simon.eszter@nytud.mta.hu}

\maketitle

%\section{Introduction}

%Az automatikus tulajdonnév-felismerés (Named Entity Recognition, NER) a természetes nyelv feldolgozását célzó alkalmazások közül az egyik legnépszerűbb, mivel hatékonyan automatizálható, és eredménye hasznos bemenete különböző magasabb szintű információkinyerő és -feldolgozó rendszereknek. A feladat során strukturálatlan szövegben kell azonosítani és az előre definiált osztályok valamelyikébe besorolni a neveket. Az egyik leggyakrabban alkalmazott nemzetközi sztenderd a CoNLL-2003 verseny kiírását követi, amely szerint a négy névkategória a következő: személy-, intézmény-, földrajzi név és az ezek egyikébe sem tartozó egyéb kategória. 

%Named Entity Recognition (NER), the task of identifying Named Entities (NEs) in unstructured texts and classifying them into pre-selected classes, is one of the most important subtasks in many NLP tasks, such as information retrieval, information extraction or machine translation. MUC and CoNLL datasets and annotation schemes have been the major standards applied in the field of NER. 

Az automatikus tulajdonnév-felismerés (NER) során alkalmazott gold standard korpuszok általában műfajspecifikusak (főleg rövidhírek), és korlátozott méretűek. Egy kellően robusztus alkalmazás építéséhez viszont nagy méretű és változatos nyelvi anyagot tartalmazó adathalmazra van szükség. A tanítókorpuszok előállítása hagyományosan kézi annotálás útján történik, ami meglehetősen sok időt, szakértelmet és pénzt kíván, vagyis kardinális kérdés az erőforrások minimalizálása. Nagyobb tanítóhalmazhoz juthatunk, ha már meglevő korpuszokat gyúrunk össze; ebben az esetben viszont azzal kell szembesülnünk, hogy ezek más és más annotációs útmutató alapján készültek, továbbá nem azonos címkekészletet használnak. 

Ezen problémák kiküszöbölésére egy lehetséges módszer, ha automatikusan állítunk elő  
adathalmazokat, vagy pedig olyan közösségi tartalmakat használunk fel, mint pl. a Wikipedia, a Wiktionary vagy a DBPedia. Cikkünkben ezek kombinációját mutatjuk be: a Wikipediából előállított, automatikusan tulajdonnév-annotált korpuszt.  

%The standard datasets are highly domain-specific (mostly newswire) and are
%restricted in size. Researchers attempting to merge these datasets to get a bigger training corpus are faced with the
%problem of combining different tagsets and annotation schemes. Manually
%annotating large amounts of text with linguistic information is a
%time-consuming, highly skilled and delicate job, but large, accurately
%annotated corpora are essential for building robust supervised machine
%learning NER systems. Therefore, reducing the annotation cost is a key
%challenge.

%One approach is to generate the resources automatically, another one is to use
%collaborative annotation and/or collaboratively constructed resources, such as
%Wikipedia, Wiktionary, Linked Open Data, or DBpedia. In this paper we combine
%these approaches by automatically generating freely available NE tagged
%corpora from Wikipedia.

%\section{Wikipedia and NER}

A Wikipediát, mely a legismertebb, önkéntesek által szerkesztett nyílt internetes enciklopédia, számos NLP feladathoz sikeresen alkalmazták már. Mivel ez az egyik legnagyobb elérhető forrása az entitásoknak, az automatikus tulajdonnév-felismerés teljesítményének növelésére több módon is kiválóan használható. Nyilvánvaló felhasználási módja különböző névlisták kinyerése (pl. \cite{Toral_2006.a}); de a Wikipediában található tudás a tanuló algoritmus számára használható jegyek formájában is beépíthető felügyelt tanulási rendszerekbe (pl. \cite{KaTo07}). 

Számos példát találunk arra is a nemzetközi irodalomban, amikor a Wikipediából építenek korpuszt, amelyben az entitásokra vonatkozó információt valamilyen formában megőrzik és feldolgozzák. Ezt a megközelítést követtük mi is, de az eddig alkalmazott módszerek helyett, melyek elsősorban a cikkek definíciójából nyerték ki az entitás kategóriájára vonatkozó információt, mi a DBPedia ontológiai osztályait képeztük le a sztenderd CoNLL névosztályokra, majd kötöttük őket Wikipédia-entitásokhoz. Az általunk létrehozott feldolgozólánc elemei nagyrészt nyelvfüggetlenek, vagyis módszerünkkel létrehozhatók tanítókorpuszok kevesebb erőforrással rendelkező nyelvekre is. Cikkünkben a feldolgozás folyamatát és eredményét: egy magyar és egy angol nyelvű automatikusan tulajdonnév-annotált korpuszt mutatunk be.

%Wikipedia (WP, see {\tt http://wikipedia.org}), a free
%multilingual Internet encyclopedia, written collaboratively by volunteers, is
%a goldmine of information.
%WP has been applied to several NLP tasks such as word sense
%disambiguation, ontology and thesaurus building, and question answering (see
%Medelyan et al.~\cite{Medelyan:09} for a survey). It is recognized as one
%of the largest available collections of entities, and also as a resource that
%can improve the accuracy of NER. The most obvious utilization of WP for NER is
%extracting gazetteers containing person names, locations or organizations
%(e.g.~Toral and Mu\~noz \cite{Toral:06}). Creating dictionaries of
%entities is also a common step of NE disambiguation
%\cite{Bunescu:06,Cucerzan:07}. Both supervised and unsupervised NER systems
%use such lists, see e.g.~Nadeau et al.~\cite{Nadeau:06} The knowledge
%embodied in WP may also be incorporated in NER learning as features,
%e.g.~Kazama and Torisawa \cite{Kazama:07} showed that automatic
%extraction of category labels from WP improves the accuracy of a supervised NE
%tagger.

%Another approach to improve NER with WP is the automatic creation of training
%data. Nothman et al.~\cite{Nothman:08} used a similar method to create
%a NE annotated text in English. They transformed the WP links into NE
%annotations by classifying the target articles into standard entity
%classes. Their approach to classification is based primarily on category head
%nouns and the opening sentences of articles where definitions are often given.

%Our approach to recognize and classify NEs in corpora generated from WP was to
%map the DBpedia ontology classes to standard NE tags and assign these to WP
%entities. the one presented here is the first automatically NE annotated corpus for Hungarian.

\section{Creating the Corpus}
\subsection{Creating the English Corpus}

\section{Data description}

A korpusz a Wikipediához hasonlóan a Creative Commons Attribution-Sharealike
3.0 Unported (CC-BY-SA) licensz alatt érhető el. Szabadon letölthető a
\texttt{http://hlt.sztaki.hu} oldalról és a META-SHARE tárhelyről. Utóbbi egy nyílt,
elosztott hálózat, amelyet a nyelvi erőforrások megosztására hoztak létre a META-NET-en,
egy, az Európai Bizottság által támogatott Kiválósági Hálózaton belül.

A korpusz vertikális formátumú (\texttt{.tsv}) szövegfájlok alkotják. Egy sor külön mezőkben tartalmazza a tokent, annak lemmatizált formájat és szófaját, illetve a NER címkéjét. Az utóbbi három mező közül csak a NER címkék ``ezüst sztenderd" minőségűek --- a többi nyelvi információt statisztikai eszközökkel generáltuk.

\section{Evaluation}

\section{Conclusion}

\cite{Nothman:08} \cite{Szarvas:06} \cite{Medelyan:09}

%
% ---- Bibliography ----
%
\bibliographystyle{huplain}
\bibliography{abstract}

\end{document}
