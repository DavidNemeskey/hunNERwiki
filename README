Scripts:
=======

extract_dbpedia_types.py:
------------------------
Takes the instance file as input and extracts the article - NER category
pairings. The mapping from DBPedia to ConLL categories can be supplied in
another file.

ner_filter_incomplete_sentences.py:
----------------------------------
Filters incomplete sentences. To be applied on the tagged training corpus.

Scripts to run:
--------------

## To parse a Wikipedia dump:
python parse_insert_into_rows.py < /home/ndavid/huwiki/huwiki-latest-page.sql > /home/ndavid/huwiki/pages
awk '$2 == 0 {print $3}' pages | sed -e "s/_/ /g" > main_pages
python /home/ndavid/../zseder/Proj/WP/AutoCorpus/articles.py < huwiki-latest-pages-articles.xml > huwiki-latest-pages-articles.out
python split_records.py huwiki-latest-pages-articles.out "^%%#PAGE.*" 4
# (modify wiki.conf accordingly)
python create_morphtable.py -l hu wiki.conf /home/ndavid/huwiki/huwiki-morph.4 /home/ndavid/huwiki/huwiki-latest-pages-articles.out.4 >/home/ndavid/huwiki/morph-log.4 2>&1
# (for pid in `ps | grep python | awk '{print $1}'`; do renice -n 10 -p $pid; done)
python wikitext_to_conll.py -l hu.3 wiki.conf /home/ndavid/huwiki/huwiki-latest-pages-articles.out.3 /home/ndavid/huwiki/huwiki-articles.conll.3 /home/ndavid/huwiki/huwiki-templates.3 > ~/huwiki/log.3 2>&1
# (for pid in `ps | grep python | awk '{print $1}'`; do renice -n 10 -p $pid; done)

## To create the entity list:
python2.7 scripts/extract_dbpedia_types.py -c data/dbpedia_3.7.owl -m data/dbpedia_map data/instance_types_hu.nt > data/nerlist.hu.en
# Interlanguage links: hu -> all (to resolve 
python extract_langlinks.py /home/ndavid/huwiki/huwiki-20120309-langlinks.sql /home/ndavid/huwiki/pages > /home/ndavid/huwiki/hu_to_all_pages
# Interlanguage links: en -> hu
python extract_langlinks.py -l en -r /home/ndavid/huwiki2/huwiki-20120309-langlinks.sql /home/ndavid/huwiki/pages > /home/ndavid/huwiki/en_to_hu_pages
# Redirects (hu)
python extract_redirects.py -l hu -r /home/ndavid/huwiki2/huwiki-20120309-redirect.sql /home/ndavid/huwiki/pages > /home/ndavid/huwiki/normal_to_redir_pages

## To extract date pages (NER category 0):
# Hungarian
python extract_date_pages.py -l hu /home/ndavid/git/langtools/Scripts/langtools/wikipedia/wiki.conf /home/ndavid/huwiki/huwiki-articles.conll.* > date_pages
or
# English
python scripts/extract_date_pages_from_templates.py -l en /home/ndavid/git/langtools/Scripts/langtools/wikipedia/wiki.conf /mnt/store/home/ndavid/enwiki/templates/wikipedia.*.conll.templates > /mnt/store/home/ndavid/enwiki/date_pages 2> log

## To expand the entity list:
# Hungarian
# To Hungarian pages
python scripts/ner_list_to_hu.py data/nerlist.hu.en data/en_to_hu_pages > data/nerlist.hu.hu
# Get the corresponding pages in all languages
python scripts/ner_list_to_hu.py data/nerlist.hu.hu data/hu_to_all_pages > data/nerlist.hu.all
# Hungarian redirects
python scripts/ner_list_to_hu.py data/nerlist.hu.hu data/normal_to_redir_pages > nerlist.hu.redirects
# Now to round it up...
export LC_ALL=C
cat data/nerlist.hu.en data/nerlist.hu.hu data/nerlist.hu.all data/nerlist.hu.redirects data/date_pages | sort -u > data/nerlist.hu
# English
# Get the corresponding pages in all languages
python scripts/ner_list_to_hu.py /mnt/store/home/ndavid/enwiki/nerlist.en.en /mnt/store/home/ndavid/enwiki/en_to_all_pages > /mnt/store/home/ndavid/enwiki/nerlist.en.all
# Redirects
python scripts/ner_list_to_hu.py /mnt/store/home/ndavid/enwiki/nerlist.en.en /mnt/store/home/ndavid/enwiki/normal_to_redir_pages > /mnt/store/home/ndavid/enwiki/nerlist.en.redirects
# Now to round it up...
export LC_ALL=C
cat /mnt/store/home/ndavid/enwiki/nerlist.en.en /mnt/store/home/ndavid/enwiki/nerlist.en.all /mnt/store/home/ndavid/enwiki/nerlist.en.redirects /mnt/store/home/ndavid/enwiki/date_pages | sort -u > /mnt/store/home/ndavid/enwiki/nerlist.en


## To Extract the training sentences:
# Hungarian
for i in `seq 4`; do python scripts/select_ner_training_hu.py -d -f -l hu -r data/normal_to_redir_pages -r data/hu_to_all_pages /home/ndavid/git/langtools/Scripts/langtools/wikipedia/wiki.conf data/nerlist.hu /home/ndavid/huwiki/huwiki-articles.conll.$i > out.$i 2>&1 & done
# English
python scripts/select_ner_training_en.py -d -f -l en -r /mnt/store/home/ndavid/enwiki/normal_to_redir_pages -r /mnt/store/home/ndavid/enwiki/en_to_all_pages -t /mnt/store/home/ndavid/enwiki/titles.en /home/ndavid/git/langtools/Scripts/langtools/wikipedia/wiki.conf /mnt/store/home/ndavid/enwiki/nerlist.en /mnt/store3/zseder/WP/2011/Conll.new/wikipedia.211.conllnew > out.211 2>&1

Data files:
==========

dbpedia_map:
-----------
Mapping from DBPedia's categories to ConLL's.
- MusicGenre: titles/types, but not mentioned explicitly.
- Eras in time: should this include Year or YearInSpaceFlight?
- EthnicGroup: plural
- title may contain extra words, punctuation marks, plural case, etc.
- AutomobileEngine?
- British -> United Kingdom: LOC?!

dbpedia_3.x.owl:
---------------
The T-box of the DBPedia ontology. Downloaded from DBPedia.

instance_types_en.nt:
--------------------
Instance type data. Downloaded from DBPedia.

dbpedia_entities:
----------------
Output of the extract_dbpedia_types.py when run on instance_types_en.nt.
Used in generation of the NER training corpus.

Abbreviations:
-------------
http://www.learn-english-today.com/business-english/abbreviations.html
http://www.indiana.edu/~letrs/help-services/QuickGuides/oed-abbr.html

Files we need from Wikipedia:
----------------------------
- all pages (hu: Articles only!)
- huwiki-20120309-page.sql.gz
- huwiki-20120309-langlinks.sql.gz
- huwiki-20120309-redirect.sql.gz

