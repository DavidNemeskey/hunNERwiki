Scripts:
=======

extract_dbpedia_types.py:
------------------------
Takes the instance file as input and extracts the article - NER category
pairings. The mapping from DBPedia to ConLL categories can be supplied in
another file.

ner_filter_incomplete_sentences.py:
----------------------------------
Filters incomplete sentences. To be applied on the tagged training corpus.

Scripts to run:
--------------

To create the entity list:
python2.7 scripts/extract_dbpedia_types.py -c data/dbpedia_3.7.owl -m data/dbpedia_map data/instance_types_hu.nt > data/nerlist.hu.en
# Interlanguage links: hu -> all (to resolve 
python extract_langlinks.py /home/ndavid/huwiki/huwiki-20120309-langlinks.sql /home/ndavid/huwiki/pages > /home/ndavid/huwiki/hu_to_all_pages
# Interlanguage links: en -> hu
python extract_langlinks.py -l en -r /home/ndavid/huwiki2/huwiki-20120309-langlinks.sql /home/ndavid/huwiki/pages > /home/ndavid/huwiki/en_to_hu_pages
# Redirects (hu)
python extract_redirects.py -r /home/ndavid/huwiki2/huwiki-20120309-redirect.sql /home/ndavid/huwiki/pages > /home/ndavid/huwiki/normal_to_redir_pages

To parse a Wikipedia dump:
python parse_insert_into_rows.py < /home/ndavid/huwiki/huwiki-latest-page.sql > /home/ndavid/huwiki/pages
awk '$2 == 0 {print $3}' pages | sed -e "s/_/ /g" > main_pages
python /home/ndavid/../zseder/Proj/WP/AutoCorpus/articles.py < huwiki-latest-pages-articles.xml > huwiki-latest-pages-articles.out
python split_records.py huwiki-latest-pages-articles.out "^%%#PAGE.*" 4
(modify wiki.conf accordingly)
python create_morphtable.py -l hu wiki.conf /home/ndavid/huwiki/huwiki-morph.4 /home/ndavid/huwiki/huwiki-latest-pages-articles.out.4 >/home/ndavid/huwiki/morph-log.4 2>&1
(for pid in `ps | grep python | awk '{print $1}'`; do renice -n 10 -p $pid; done)
python wikitext_to_conll.py -l hu.3 wiki.conf /home/ndavid/huwiki/huwiki-latest-pages-articles.out.3 /home/ndavid/huwiki/huwiki-articles.conll.3 /home/ndavid/huwiki/huwiki-templates.3 > ~/huwiki/log.3 2>&1
(for pid in `ps | grep python | awk '{print $1}'`; do renice -n 10 -p $pid; done)

To extract date pages (NER category 0):
python extract_date_pages.py -l hu /home/ndavid/git/langtools/Scripts/langtools/wikipedia/wiki.conf /home/ndavid/huwiki/huwiki-articles.conll.* > date_pages

To expand the entity list:
# To Hungarian pages
python scripts/ner_list_to_hu.py data/nerlist.hu.en data/en_to_hu_pages > data/nerlist.hu.hu
# Get the corresponding pages in all languages
python scripts/ner_list_to_hu.py data/nerlist.hu.hu data/hu_to_all_pages > data/nerlist.hu.all
# Hungarian redirects
python scripts/ner_list_to_hu.py data/nerlist.hu.hu data/normal_to_redir_pages > nerlist.hu.redirects
# Now to round it up...
export LC_ALL=C
cat data/nerlist.hu.en data/nerlist.hu.hu data/nerlist.hu.all data/nerlist.hu.redirects data/date_pages | sort -u > data/nerlist.hu

To Extract the training sentences:
for i in `seq 4`; do python scripts/select_ner_training2.py -d -f -l hu -r data/normal_to_redir_pages -r data/hu_to_all_pages /home/ndavid/git/langtools/Scripts/langtools/wikipedia/wiki.conf data/nerlist.hu /home/ndavid/huwiki/huwiki-articles.conll.$i > out.$i 2>&1 & done

Data files:
==========

dbpedia_map:
-----------
Mapping from DBPedia's categories to ConLL's.
- MusicGenre: titles/types, but not mentioned explicitly.
- Eras in time: should this include Year or YearInSpaceFlight?
- EthnicGroup: plural
- title may contain extra words, punctuation marks, plural case, etc.
- AutomobileEngine?
- British -> United Kingdom: LOC?!

dbpedia_3.x.owl:
---------------
The T-box of the DBPedia ontology. Downloaded from DBPedia.

instance_types_en.nt:
--------------------
Instance type data. Downloaded from DBPedia.

dbpedia_entities:
----------------
Output of the extract_dbpedia_types.py when run on instance_types_en.nt.
Used in generation of the NER training corpus.
